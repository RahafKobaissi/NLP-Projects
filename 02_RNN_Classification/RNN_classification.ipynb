{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sentences = [\n",
    "    \"I love this movie\", \"This film is great\", \"Amazing experience\",\n",
    "    \"I enjoyed it\", \"Best movie ever\", \"I hate this movie\", \n",
    "    \"This film is terrible\", \"Awful experience\", \"I disliked it\", \n",
    "    \"Worst movie ever\"\n",
    "]\n",
    "labels = torch.tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary and tokenization\n",
    "vocab = {\"<unk>\": 0}\n",
    "tokenized_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "        tokenized_sentence.append(vocab[word])\n",
    "    tokenized_sentences.append(torch.tensor(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2, 3, 4]),\n",
       " tensor([3, 5, 6, 7]),\n",
       " tensor([8, 9]),\n",
       " tensor([ 1, 10, 11]),\n",
       " tensor([12,  4, 13])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "embedding_dim = 10\n",
    "hidden_dim = 8\n",
    "num_layers = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "fc = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.Adam(list(embedding.parameters()) + list(rnn.parameters()) + list(fc.parameters()), lr=learning_rate)\n",
    "criterion = nn.BCELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0734, -1.0112,  0.1437,  1.3157, -1.4879, -0.0434, -0.7689, -1.3871,\n",
       "         -0.8758,  1.1015],\n",
       "        [ 0.5250, -0.7598,  1.1881, -0.6249, -1.1482,  0.2297,  1.3443,  0.1197,\n",
       "          0.3731, -0.2501],\n",
       "        [-1.3194,  1.0482,  1.2214,  1.5471,  1.1125, -0.1039, -0.3671, -0.1031,\n",
       "         -0.4327,  0.0018],\n",
       "        [ 0.1261, -0.3564, -1.2284,  1.7846,  0.5955,  0.4956,  1.0887, -0.3518,\n",
       "         -0.5098, -1.4278]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.2670\n",
      "Epoch 20/100, Loss: 0.0999\n",
      "Epoch 30/100, Loss: 0.0551\n",
      "Epoch 40/100, Loss: 0.0385\n",
      "Epoch 50/100, Loss: 0.0302\n",
      "Epoch 60/100, Loss: 0.0246\n",
      "Epoch 70/100, Loss: 0.0206\n",
      "Epoch 80/100, Loss: 0.0176\n",
      "Epoch 90/100, Loss: 0.0152\n",
      "Epoch 100/100, Loss: 0.0132\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, sentence in enumerate(tokenized_sentences):\n",
    "        sentence = sentence.unsqueeze(0)  \n",
    "        label = labels[i].view(1, 1)\n",
    "        \n",
    "        embedded = embedding(sentence)\n",
    "        output, _ = rnn(embedded)\n",
    "        output_mean = output.mean(dim=1)\n",
    "        prediction = sigmoid(fc(output_mean))\n",
    "        # print(prediction)\n",
    "        loss = criterion(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(tokenized_sentences):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentence):\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        tokenized_sentence.append(vocab.get(word, vocab[\"<unk>\"]))\n",
    "    tokenized_sentence = torch.tensor(tokenized_sentence).unsqueeze(0)  \n",
    "    print(tokenized_sentence)\n",
    "    with torch.no_grad():\n",
    "        embedded = embedding(tokenized_sentence)\n",
    "        output, _ = rnn(embedded)\n",
    "        output_mean = output.mean(dim=1)\n",
    "        prediction = sigmoid(fc(output_mean))\n",
    "        print(prediction)\n",
    "        predicted_label = (prediction >= 0.5).float().item()\n",
    "        print(predicted_label)\n",
    "\n",
    "    return \"Positive\" if predicted_label == 1.0 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4, 6, 0]])\n",
      "tensor([[0.9818]])\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment('this movie is good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 0, 6, 8]])\n",
      "tensor([[0.9963]])\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment('this course is Amazing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0]])\n",
      "tensor([[0.9330]])\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment('not bad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
